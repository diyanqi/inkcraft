// utils/generate-continuation.ts
import { createOpenAI } from '@ai-sdk/openai';
import { generateText, streamText } from 'ai'; // StreamPart is implicitly handled by the SDK types now
import { parse } from 'best-effort-json-parser';

// Define AI model instances - these are shared
const openai = createOpenAI({
  baseURL: process.env.OPENAI_API_BASEURL || '',
  apiKey: process.env.OPENAI_API_APIKEY || '',
  compatibility: 'compatible',
});

const deepseek = createOpenAI({
  baseURL: process.env.DEEPSEEK_API_BASEURL || '',
  apiKey: process.env.DEEPSEEK_API_APIKEY || '',
  compatibility: 'compatible',
});

const siliconflow = createOpenAI({
  baseURL: process.env.SILICONFLOW_API_BASEURL || '',
  apiKey: process.env.SILICONFLOW_API_APIKEY || '',
  compatibility: 'compatible',
})

const gpt = createOpenAI({
  baseURL: process.env.GPT_API_BASEURL || '',
  apiKey: process.env.GPT_API_APIKEY || '',
  compatibility: 'compatible',
})
// Helper type for the enqueue function
type EnqueueFunction = (data: any) => void;

// Define the expected top-level scoring categories from the prompt
const SCORING_CATEGORIES = [
  "å†…å®¹ä¸åŸæ–‡èæ´½åº¦",
  "æƒ…èŠ‚åˆç†æ€§ä¸å®Œæ•´æ€§",
  "è¯æ±‡ä¸°å¯Œæ€§",
  "è¯­æ³•å‡†ç¡®æ€§",
  "å¥å¼å¤šæ ·æ€§",
  "è¡”æ¥ä¸è¿è´¯æ€§",
  "åˆ›æ–°æ€§ä¸é€»è¾‘æ€§",
  "æ–‡ä½“ä¸äººç§°ä¸€è‡´æ€§",
  "è‹±è¯­æ–‡å­¦ç´ å…»ä¸æ•™å¸ˆä¸»è§‚è¯„ä»·"
];


/**
 * Generates the score and detailed breakdown for the essay continuation.
 * Streams the raw AI response to the console and sends progress updates for scoring categories.
 * @param originalText The original text prompt.
 * @param essayText The user's essay continuation text.
 * @param tone The tone parameter.
 * @param model The model parameter.
 * @param enqueue Function to send progress updates via the stream.
 * @returns An object containing the calculated score and the detailed content string.
 */
export async function generateScore(
  originalText: string,
  essayText: string,
  tone: string,
  model: string,
  enqueue: EnqueueFunction
): Promise<{ score: number, content: string }> {
  enqueue({ type: 'progress', message: 'æ­£åœ¨ç”Ÿæˆè¯„åˆ†...' });
  console.log("Initiating score generation..."); // Console log start

  let content = `# 1. é¢˜ç›®\n${originalText}\n# 2. æˆ‘çš„ç»­å†™\n${essayText}\n`;
  let fullResponseText = ''; // Accumulator for the full AI response text
  const reportedCategories = new Set<string>(); // Track categories already reported via enqueue

  try {
    // Use the specific model defined in the original logic
    const aiModel = (
      model === 'llama' ? openai('@cf/meta/llama-4-scout-17b-16e-instruct') :
        model === 'deepseek' ? deepseek('deepseek-chat') :
          model === 'qwen' ? siliconflow('Qwen/Qwen3-8B') :
            model === 'glm' ? siliconflow('THUDM/GLM-Z1-9B-0414') :
              model === 'gpt4' ? siliconflow('gpt-3.5-turbo') :
                openai('@cf/qwen/qwen1.5-14b-chat-awq')
    );

    const tonePrompt = (
      tone === 'serious' ? "ç‰¹åˆ«è¦æ±‚ï¼šä½ çš„è¯„è®ºæ‰¹æ”¹è¯­æ°”éœ€è¦ä¸€æœ¬æ­£ç»ï¼Œåƒä¸€ä¸ªç»éªŒä¸°å¯Œè€Œä¸¥å‰çš„è€å¸ˆã€‚" :
        tone === 'humorous' ? "ç‰¹åˆ«è¦æ±‚ï¼šä½ çš„è¯„è®ºæ‰¹æ”¹è¯­æ°”éœ€è¦å¹½é»˜é£è¶£ï¼Œå¯ä»¥ç”¨ä¸Šç½‘ç»œæ¢—ï¼Œå¯ä»¥å¤šç”¨emojiè¡¨æƒ…åŒ…ï¼Œå¯ä»¥å¤šæç¬‘ã€‚" :
          tone === 'sharp' ? "ç‰¹åˆ«è¦æ±‚ï¼šä½ çš„è¯„è®ºæ‰¹æ”¹è¯­æ°”éœ€è¦å°–é”ã€é”è¯„ã€ä¸€é’ˆè§è¡€ã€‚" :
            ""
    );

    console.log("--- Streaming AI response for score ---"); // Marker for console output

    // Call streamText and iterate over the response stream
    const streamResult = await streamText({
      model: aiModel,
      messages: [
        {
          role: 'system',
          content: `"ä½ æ˜¯ä¸€ä¸ªé«˜è€ƒè‹±è¯­è¯»åç»­å†™çš„é˜…å·è€å¸ˆï¼Œç°åœ¨æœ‰ä¸€ä¸ªé«˜è€ƒè‹±è¯­è¯»åç»­å†™ä½œæ–‡é¢˜ç›®å’Œä¸€ç¯‡å¾…æ‰¹æ”¹ç»­å†™ä½œæ–‡ï¼Œéœ€è¦ä½ å¯¹è¿™ç¯‡å¾…æ‰¹æ”¹ä½œæ–‡è¿›è¡Œè¯„åˆ†ã€‚
è¦æ±‚ï¼š
1ï¼‰è¯·è®¤çœŸé˜…è¯»ä½œæ–‡æ‰¹æ”¹è¦æ±‚å’Œä½œæ–‡é¢˜ç›®ï¼Œå¯¹è¿™ç¯‡å¾…æ‰¹æ”¹ä½œæ–‡è¿›è¡Œå…¬æ­£ä¸¥æ ¼çš„æ‰¹æ”¹å’Œæ‰“åˆ†ï¼›
2ï¼‰è¯„åˆ†ä¸€å®šè¦ä¸¥æ ¼ï¼Œå®äº‹æ±‚æ˜¯ï¼Œå¥½çš„ç»™é«˜åˆ†ï¼Œå·®çš„ç»™ä½åˆ†ã€‚
3ï¼‰æœ€åè¿”å›å†…å®¹è¦ä¸¥æ ¼æŒ‰ç…§æœ€åçš„è¾“å‡ºæ ¼å¼ã€‚

ä¸€ã€ä½œæ–‡æ‰¹æ”¹è¦æ±‚
  ä¸èƒ½è½»æ˜“éšä¾¿ç»™å‡ºé«˜åˆ†ï¼Œæ›´ä¸èƒ½ç»™å‡ºæ»¡åˆ†ã€‚
  è¦ä¸¥æ ¼æ‰¹æ”¹ï¼Œæ¯ä¸€å¤„è¯„åˆ†éƒ½è¦åœ¨ç”¨æˆ·çš„ä½œæ–‡ä¸­æ‰¾åˆ°è¯æ®ã€‚
  æ¯ä¸ªå­é¡¹éœ€å…ˆè¯´æ˜è¯„åˆ†ç†ç”±å†ç»™å‡ºå…·ä½“åˆ†æ•°ï¼š
1. å†…å®¹ä¸åŸæ–‡èæ´½åº¦ï¼ˆ15åˆ†ï¼‰
â€¢ 1.1 æƒ…èŠ‚è¿è´¯æ€§ï¼ˆ5åˆ†ï¼‰
  â—¦ 5åˆ†ï¼šæ®µé¦–ç”¨æ—¶é—´/åŠ¨ä½œ/ç¯å¢ƒæå†™æ— ç¼è¡”æ¥åŸæ–‡ï¼ˆå¦‚åŸæ–‡ç»“å°¾"midnight"â†’ç»­å†™"At dawn..."ï¼‰
  â—¦ 2åˆ†ï¼šè¡”æ¥åŸºæœ¬åˆç†ä½†ç¼ºä¹è¿‡æ¸¡è¯ï¼ˆç›´æ¥è·³è½¬"Two days later..."ï¼‰
  â—¦ 0åˆ†ï¼šæƒ…èŠ‚æ–­è£‚ï¼ˆå¦‚åŸæ–‡ç´§å¼ åœºæ™¯çªè½¬æ¬¢ä¹æ´¾å¯¹ï¼‰
â€¢ 1.2 äººç‰©ä¸€è‡´æ€§ï¼ˆ5åˆ†ï¼‰
  â—¦ 5åˆ†ï¼šè¨€è¡Œç¬¦åˆåŸæ–‡è®¾å®šï¼ˆå¦‚æ‡¦å¼±è§’è‰²ä¿æŒè°¨æ…ï¼‰
  â—¦ 2åˆ†ï¼šæ¬¡è¦äººç‰©æ€§æ ¼è½»å¾®åå·®ï¼ˆå¦‚æœ‹å‹ä»å®‰é™å˜ä¸ºå¥è°ˆï¼‰
  â—¦ 0åˆ†ï¼šä¸»è§’è¡Œä¸ºä¸¥é‡çŸ›ç›¾ï¼ˆå–„è‰¯è€…æ— æ•…ä¼¤å®³ä»–äººï¼‰
â€¢ 1.3 ä¸»é¢˜å¥‘åˆåº¦ï¼ˆ5åˆ†ï¼‰
  â—¦ 5åˆ†ï¼šæ·±åŒ–åŸæ–‡ä¸»é¢˜ï¼ˆå¦‚"ç¯ä¿"â†’ç»­å†™æ¸…ç†æ±¡æŸ“ï¼‰
  â—¦ 2åˆ†ï¼šä¿æŒä¸»é¢˜ä½†ç¼ºä¹å‘å±•ï¼ˆä»…é‡å¤åŸæ–‡å†…å®¹ï¼‰
  â—¦ 0åˆ†ï¼šåç¦»æ ¸å¿ƒä¸»é¢˜ï¼ˆå¦‚"å‹è°Š"æ•…äº‹çªè½¬å•†ä¸šç«äº‰ï¼‰
2. æƒ…èŠ‚åˆç†æ€§ä¸å®Œæ•´æ€§ï¼ˆ15åˆ†ï¼‰
â€¢ 2.1 å†²çªè§£å†³ï¼ˆ5åˆ†ï¼‰
  â—¦ 5åˆ†ï¼šç”¨ä¼ç¬”è§£å†³çŸ›ç›¾ï¼ˆå¦‚ç”¨å‰æ–‡"åœ°å›¾"è„±å›°ï¼‰
  â—¦ 2åˆ†ï¼šè§£å†³åˆç†ä½†ç¼ºä¹é“ºå«ï¼ˆçªç„¶å‡ºç°æ•‘æ´é˜Ÿï¼‰
  â—¦ 0åˆ†ï¼šæœªè§£å†³å†²çªæˆ–æœºæ¢°é™ç¥ï¼ˆå¤–æ˜Ÿäººæ•‘åœºï¼‰
â€¢ 2.2 ç¬¦åˆå¸¸è¯†ï¼ˆ5åˆ†ï¼‰
  â—¦ 5åˆ†ï¼šç¬¦åˆç‰©ç†/ç¤¾ä¼šè§„åˆ™ï¼ˆé›¨å¤©ç”¨å¡‘æ–™å¸ƒç”Ÿç«ï¼‰
  â—¦ 2åˆ†ï¼šè½»å¾®ç‘•ç–µï¼ˆæ£®æ—ä¸­è½»æ˜“æ‰¾åˆ°è¯å“ï¼‰
  â—¦ 0åˆ†ï¼šä¸¥é‡è¿èƒŒå¸¸è¯†ï¼ˆå¾’æ‰‹å‡»é€€ç‹¼ç¾¤ï¼‰
â€¢ 2.3 ç§¯æç»“å±€ï¼ˆ5åˆ†ï¼‰
  â—¦ 5åˆ†ï¼šè‡ªç„¶ä¼ é€’æ­£èƒ½é‡ï¼ˆäº’åŠ©è„±å›°åå»ºç«‹å‹è°Šï¼‰
  â—¦ 2åˆ†ï¼šç»“å±€ç§¯æä½†çªå…€ï¼ˆç›´æ¥é™ˆè¿°"ä»–ä»¬å¹¸ç¦äº†"ï¼‰
  â—¦ 0åˆ†ï¼šæ¶ˆæç»“å±€ï¼ˆä¸»è§’æ”¾å¼ƒæˆ–æ­»äº¡ï¼‰
3. è¯æ±‡ä¸°å¯Œæ€§ï¼ˆ10åˆ†ï¼‰
â€¢ 3.1 è¯æ±‡å¤šæ ·æ€§ï¼ˆ3åˆ†ï¼‰
  â—¦ 3åˆ†ï¼šåŒä¹‰è¯æ›¿æ¢â‰¥5å¤„ï¼ˆ"said"â†’whispered/exclaimedï¼‰
  â—¦ 1åˆ†ï¼šæ›¿æ¢3-4å¤„
  â—¦ 0åˆ†ï¼šé‡å¤ä½¿ç”¨åŸºç¡€è¯æ±‡
â€¢ 3.2 ç”¨è¯å‡†ç¡®æ€§ï¼ˆ3åˆ†ï¼‰
  â—¦ 3åˆ†ï¼šæ— ä¸­å¼è‹±è¯­ä¸”æ­é…ç²¾å‡†ï¼ˆ"make a decision"ï¼‰
  â—¦ 1åˆ†ï¼š1-2å¤„é”™è¯¯ï¼ˆè¯¯ç”¨"borrow"ä»£æ›¿"lend"ï¼‰
  â—¦ 0åˆ†ï¼šâ‰¥3å¤„æ­é…é”™è¯¯
â€¢ 3.3 é«˜çº§è¯æ±‡ï¼ˆ4åˆ†ï¼‰
  â—¦ 4åˆ†ï¼šC1+è¯æ±‡â‰¥3ä¸ªï¼ˆresilient, dilemmaï¼‰
  â—¦ 2åˆ†ï¼šä½¿ç”¨1-2ä¸ª
  â—¦ 0åˆ†ï¼šä»…åŸºç¡€è¯æ±‡
4. è¯­æ³•å‡†ç¡®æ€§ï¼ˆ15åˆ†ï¼‰
â€¢ 4.1 ä¸»è°“ä¸€è‡´ï¼ˆ5åˆ†ï¼‰
  â—¦ 5åˆ†ï¼šé›¶é”™è¯¯
  â—¦ 2åˆ†ï¼š1-2å¤„é”™è¯¯ï¼ˆ"She don't know"ï¼‰
  â—¦ 0åˆ†ï¼šâ‰¥3å¤„é”™è¯¯
â€¢ 4.2 æ—¶æ€æ­£ç¡®ï¼ˆ5åˆ†ï¼‰
  â—¦ 5åˆ†ï¼šå…¨æ–‡æ—¶æ€ä¸åŸæ–‡100%ä¸€è‡´
  â—¦ 2åˆ†ï¼š1-2å¤„æ—¶æ€è·³è·ƒï¼ˆè¿‡å»â†’ç°åœ¨æ—¶ï¼‰
  â—¦ 0åˆ†ï¼šâ‰¥3å¤„æ—¶æ€é”™è¯¯
â€¢ 4.3 å¥å­ç»“æ„ï¼ˆ5åˆ†ï¼‰
  â—¦ 5åˆ†ï¼šæ— ç²˜è¿å¥/ç‰‡æ®µå¥
  â—¦ 2åˆ†ï¼š1å¤„ç»“æ„é”™è¯¯ï¼ˆæ— è¿è¯é€—å·è¿æ¥ï¼‰
  â—¦ 0åˆ†ï¼šâ‰¥2å¤„ç»“æ„é”™è¯¯
5. å¥å¼å¤šæ ·æ€§ï¼ˆ10åˆ†ï¼‰
â€¢ 5.1 å¤åˆå¥å æ¯”ï¼ˆ4åˆ†ï¼‰
  â—¦ 4åˆ†ï¼šå¤åˆå¥â‰¥40%ï¼ˆå¦‚å®šè¯­ä»å¥"who..."ï¼‰
  â—¦ 2åˆ†ï¼š30-39%
  â—¦ 0åˆ†ï¼šï¼œ30%
â€¢ 5.2 ç‰¹æ®Šå¥å¼ï¼ˆ6åˆ†ï¼‰
  â—¦ 6åˆ†ï¼šä½¿ç”¨â‰¥3ç§ï¼ˆå€’è£…/å¼ºè°ƒ/ç‹¬ç«‹ä¸»æ ¼ï¼‰
  â—¦ 2åˆ†ï¼šä½¿ç”¨1-2ç§
  â—¦ 0åˆ†ï¼šæœªä½¿ç”¨
6. è¡”æ¥ä¸è¿è´¯æ€§ï¼ˆ10åˆ†ï¼‰
â€¢ 6.1 æ˜¾æ€§è¡”æ¥ï¼ˆ6åˆ†ï¼‰
  â—¦ 6åˆ†ï¼šè¿‡æ¸¡è¯ä½¿ç”¨â‰¥3æ¬¡ä¸”æ°å½“ï¼ˆhowever/meanwhileï¼‰
  â—¦ 3åˆ†ï¼šä½¿ç”¨1-2æ¬¡
  â—¦ 1åˆ†ï¼šæœªä½¿ç”¨æˆ–è¯¯ç”¨ï¼ˆç”¨"therefore"è¡¨è½¬æŠ˜ï¼‰
â€¢ 6.2 éšæ€§è¡”æ¥ï¼ˆ4åˆ†ï¼‰
  â—¦ 4åˆ†ï¼šä»£è¯æŒ‡ä»£100%æ¸…æ™°ï¼ˆ"they"æ˜ç¡®æŒ‡æ•‘æ´é˜Ÿï¼‰
  â—¦ 2åˆ†ï¼š1-2å¤„æŒ‡ä»£æ¨¡ç³Š
  â—¦ 0åˆ†ï¼šâ‰¥3å¤„æŒ‡ä»£æ··ä¹±
7. åˆ›æ–°æ€§ä¸é€»è¾‘æ€§ï¼ˆ10åˆ†ï¼‰
â€¢ 7.1 ç»†èŠ‚åˆ›æ–°ï¼ˆ5åˆ†ï¼‰
  â—¦ 5åˆ†ï¼šæ·»åŠ â‰¥2ä¸ªåˆç†æ–°å…ƒç´ ï¼ˆé”ˆæŒ‡å—é’ˆ/é‡æœå……é¥¥ï¼‰
  â—¦ 2åˆ†ï¼šæ·»åŠ 1ä¸ªæ–°å…ƒç´ 
  â—¦ 0åˆ†ï¼šæ— æ–°ç»†èŠ‚æˆ–è„±ç¦»é€»è¾‘
â€¢ 7.2 å› æœé€»è¾‘ï¼ˆ5åˆ†ï¼‰
  â—¦ 5åˆ†ï¼šäº‹ä»¶å› æœé“¾å®Œæ•´ï¼ˆå—ä¼¤â†’æ¶ˆæ¯’â†’æ¢å¤ï¼‰
  â—¦ 2åˆ†ï¼šå› æœåŸºæœ¬åˆç†ä½†ç¼ºæ­¥éª¤ï¼ˆç›´æ¥åŒ…æ‰æœªæ¸…æ´ï¼‰
  â—¦ 0åˆ†ï¼šå› æœæ–­è£‚ï¼ˆæœªè§£é‡Šå¦‚ä½•è·æ•‘ï¼‰
8. æ–‡ä½“ä¸äººç§°ä¸€è‡´æ€§ï¼ˆ10åˆ†ï¼‰
â€¢ 8.1 å™è¿°è§†è§’ï¼ˆ5åˆ†ï¼‰
  â—¦ 5åˆ†ï¼šä¸¥æ ¼ä¿æŒåŸæ–‡äººç§°ï¼ˆç¬¬ä¸€äººç§°å…¨ç¨‹"æˆ‘"ï¼‰
  â—¦ 3åˆ†ï¼š1-2å¤„äººç§°åç§»ï¼ˆæ··ç”¨"I"å’Œ"he"ï¼‰
  â—¦ 0åˆ†ï¼šå½»åº•æ”¹å˜è§†è§’
â€¢ 8.2 è¯­ä½“é£æ ¼ï¼ˆ5åˆ†ï¼‰
  â—¦ 5åˆ†ï¼š100%ç¬¦åˆæ•…äº‹æ–‡ä½“ï¼ˆå¯¹è¯ç”¨å¼•å·ï¼Œæ— å­¦æœ¯è¯æ±‡ï¼‰
  â—¦ 3åˆ†ï¼š1-2å¤„é£æ ¼ä¸ç¬¦ï¼ˆç”¨"furthermore"æ›¿ä»£"then"ï¼‰
  â—¦ 0åˆ†ï¼šæ–‡ä½“æ··æ‚
9. è‹±è¯­æ–‡å­¦ç´ å…»ä¸æ•™å¸ˆä¸»è§‚è¯„ä»·ï¼ˆ10åˆ†ï¼‰
â€¢ 9.1 è‹±è¯­è¯­è¨€ç´ å…»ï¼ˆ5åˆ†ï¼‰
  â—¦ 5åˆ†ï¼šæ€»ä½“è¯»æ¥è‡ªç„¶æµç•…ï¼Œç§°å¾—ä¸Šæ˜¯åœ°é“çš„ native Englishï¼Œçœ‹ä¸å‡ºæ˜¯ä¸­å›½äººå†™çš„
  â—¦ 2åˆ†ï¼šæœ‰ä¸­æ–‡å¼è‹±è¯­çš„ç—•è¿¹
  â—¦ 0åˆ†ï¼šå®Œå…¨ä¸ç¬¦åˆè‹±è¯­è¯­è¨€ä¹ æƒ¯
â€¢ 9.2 æ•™å¸ˆä¸»è§‚è¯„ä»·ï¼ˆ5åˆ†ï¼‰
  â—¦ 5åˆ†ï¼šçœ‹èµ·æ¥å¾ˆèˆ’æœï¼ŒçœŸäººé˜…å·è€å¸ˆå¯èƒ½ä¼šå–œæ¬¢
  â—¦ 2åˆ†ï¼šçœŸäººé˜…å·è€å¸ˆå¯èƒ½ä¼šè§‰å¾—æœ‰ä¸è¶³ä¹‹å¤„
  â—¦ 0åˆ†ï¼šå®Œå…¨ä¸ç¬¦åˆè€å¸ˆçš„è¯„ä»·æ ‡å‡†

äºŒã€ä½œæ–‡é¢˜ç›®
${originalText}

ä¸‰ã€ä½œæ–‡å†…å®¹
${essayText}

å››ã€è¾“å‡ºæ ¼å¼
ç¤ºä¾‹æ ‡å‡†åŒ–JSONè¾“å‡ºï¼š
{
  "åˆ†é¡¹è¯„åˆ†": {
    "å†…å®¹ä¸åŸæ–‡èæ´½åº¦": {
      "æƒ…èŠ‚è¿è´¯æ€§": {
        "reason": "æ®µé¦–ç”¨'As the rain poured'è¡”æ¥åŸæ–‡æš´é›¨åœºæ™¯ï¼Œä½†ç¼ºä¹æ—¶é—´è¿‡æ¸¡è¯",
        "score": 2
      },
      "äººç‰©ä¸€è‡´æ€§": {
        "reason": "ä¸»è§’ä¿æŒå‹‡æ•¢è®¾å®šï¼Œä½†æœ‹å‹çªç„¶å†·æ¼ ï¼ˆåŸæ–‡è®¾å®šä¸ºçƒ­æƒ…ï¼‰",
        "score": 2
      },
      "ä¸»é¢˜å¥‘åˆåº¦": {
        "reason": "æ·±åŒ–'å‹‡æ°”'ä¸»é¢˜ï¼Œé€šè¿‡å…‹æœææƒ§æ¨åŠ¨æˆé•¿",
        "score": 2
      }
    },
    "æƒ…èŠ‚åˆç†æ€§ä¸å®Œæ•´æ€§": {
      "å†²çªè§£å†³": {
        "reason": "ç”¨å‰æ–‡'å“¨å­'å‘å‡ºæ±‚æ•‘ä¿¡å·ï¼Œä¼ç¬”å›æ”¶åˆç†",
        "score": 2
      },
      "ç¬¦åˆå¸¸è¯†": {
        "reason": "'æ¹¿æœ¨å¤´ç”Ÿç«'è¿åç‡ƒçƒ§å¸¸è¯†",
        "score": 1
      },
      "ç§¯æç»“å±€": {
        "reason": "ä¼ é€’å¸Œæœ›ä½†è·æ•‘è¿‡ç¨‹ä»“ä¿ƒ",
        "score": 2
      }
    },
    "è¯æ±‡ä¸°å¯Œæ€§": {
      "è¯æ±‡å¤šæ ·æ€§": {
        "reason": "åŒä¹‰è¯æ›¿æ¢4å¤„ï¼ˆshoutedâ†’screamed/criedï¼‰",
        "score": 3
      },
      "ç”¨è¯å‡†ç¡®æ€§": {
        "reason": "è¯¯ç”¨'receive the prize'åº”ä¸º'win the prize'",
        "score": 2
      },
      "é«˜çº§è¯æ±‡": {
        "reason": "ä½¿ç”¨'resilient'å’Œ'determination'",
        "score": 2
      }
    },
    // å…¶ä»–ç»´åº¦åŒç†ï¼Œä½ éœ€è¦æ ¹æ®å‰é¢æ‰€è¯´çš„ï¼Œæ¯ä¸€æ¡éƒ½è¦è¿›è¡Œè¯„åˆ†ã€‚
  }
}
${tonePrompt}
é‡è¦æé†’ï¼šä½ çš„è¾“å‡ºä»…åŒ…å«ã€JSONæ ¼å¼ã€‘ï¼Œä¸å…è®¸å‡ºç°å…¶ä»–å­—ç¬¦æˆ–æ³¨é‡Šã€‚å¯¹äºjsonæ ¼å¼ä¸­åŒå¼•å·é‡Œçš„å†…å®¹ï¼Œè¯·å‹¿å†æ¬¡ä½¿ç”¨åŒå¼•å·ï¼Œåªå…è®¸ä½¿ç”¨å•å¼•å·ã€‚`,
        },
      ],
      maxTokens: 4096, // Reduced maxTokens slightly as a potential optimization, adjust if needed
      // temperature: 0,
      topP: 0.1,
    });

    // Iterate over the stream parts
    for await (const part of streamResult.fullStream) {
      // Check if the part is a text delta and has content
      if (part.type === 'text-delta' && part.textDelta) {
        const chunk = part.textDelta;
        // Write the chunk directly to the console's standard output
        process.stdout.write(chunk);
        // Append the chunk to the full response text
        fullResponseText += chunk;

        // --- Check for new category keys ---
        for (const category of SCORING_CATEGORIES) {
          // Check if category hasn't been reported yet AND appears in the text
          // We look for the category name enclosed in quotes followed by a colon
          // This is a heuristic and might need adjustment if AI format varies slightly
          const searchString = `"${category}":`;
          if (!reportedCategories.has(category) && fullResponseText.includes(searchString)) {
            // Send enqueue message for this category
            enqueue({ type: 'progress', message: `æ­£åœ¨è¯„åˆ†: ${category}` });
            // Mark this category as reported
            reportedCategories.add(category);
          }
        }
        // --- End check for new category keys ---
      }
      // You could potentially handle other part types here if needed
      // e.g., part.type === 'finish', part.type === 'error'
    }

    console.log("\n--- End of AI response stream ---"); // Add a newline and marker

    // Check if we received any response
    if (!fullResponseText) {
      throw new Error("AI response was empty.");
    }

    // Parse the accumulated JSON string
    const start = fullResponseText.indexOf('{');
    const end = fullResponseText.lastIndexOf('}');
    if (start === -1 || end === -1 || start >= end) {
      console.error("Invalid JSON structure received:", fullResponseText);
      throw new Error("Failed to find valid JSON object in AI response. Raw response logged.");
    }
    const jsonString = fullResponseText.substring(start, end + 1);

    let json: any;
    try {
      json = parse(jsonString); // Use best-effort parser
    } catch (parseError) {
      console.error("Failed to parse JSON:", parseError);
      console.error("Received JSON string:", jsonString);
      throw new Error(`Failed to parse AI response JSON: ${parseError}`);
    }

    // Check if the parsed JSON has the expected structure
    if (!json || typeof json !== 'object' || !json.åˆ†é¡¹è¯„åˆ†) {
      console.error("Parsed JSON missing expected 'åˆ†é¡¹è¯„åˆ†' key:", json);
      throw new Error("Parsed JSON does not have the expected structure ('åˆ†é¡¹è¯„åˆ†' key missing).");
    }

    // Calculate total score and format content
    let totalScore = 0;
    content += `# 3. è¯„åˆ†ç»†åˆ™\n`;
    // Ensure consistent order matching SCORING_CATEGORIES if possible, otherwise use the order from JSON
    const finalCategories = json.åˆ†é¡¹è¯„åˆ†; // Use the parsed JSON structure
    for (const key in finalCategories) { // Iterate through the keys from the actual JSON response
      if (Object.prototype.hasOwnProperty.call(finalCategories, key)) { // Ensure key is own property
        const section = finalCategories[key];
        // Format category header (e.g., make it bold markdown)
        content += `\n- **${key}**\n\n  `; // Add bold markdown and indent reasons
        let reasonContent = "";
        if (typeof section === 'object' && section !== null) { // Check if section is an object
          for (const subKey in section) {
            if (Object.prototype.hasOwnProperty.call(section, subKey)) { // Ensure subKey is own property
              const scoreValue = Number(section[subKey]?.score) || 0; // Ensure score is a number
              totalScore += scoreValue;
              // Combine reason and score for each sub-item if needed, or just reasons
              reasonContent += `${section[subKey]?.reason || 'N/A'}ï¼› `; // Add subkey/score detail
            }
          }
        } else {
          console.warn(`Section '${key}' is not an object or is null in the response.`);
          reasonContent = 'è¯„åˆ†æ•°æ®æ ¼å¼é”™è¯¯ï¼› ';
        }
        // Remove trailing semicolon and space, add period.
        content += reasonContent.trim().replace(/ï¼›$/, 'ã€‚') + '\n';
      }
    }


    // Convert to 25-point scale and round to one decimal place
    let score = Number((totalScore / 100 * 25).toFixed(1));

    // Fallback using regex - Keep this as a safety net
    if (isNaN(score) || (score === 0 && totalScore === 0 && fullResponseText.length > 10)) { // Added length check to avoid fallback on genuinely empty/failed responses
      console.warn("Initial score calculation resulted in 0 or NaN, attempting regex fallback...");
      const regex = /(\d+)\s*åˆ†/g;
      const matches = fullResponseText.match(regex);
      let fallbackSum = 0;
      if (matches) {
        fallbackSum = matches.reduce((acc, cur) => {
          const num = parseInt(cur, 10);
          return acc + (isNaN(num) ? 0 : num);
        }, 0);
        console.log(`Regex fallback found scores summing to: ${fallbackSum}`);
      } else {
        console.warn("Regex fallback found no score matches.");
      }
      if (fallbackSum > 0) {
        totalScore = fallbackSum;
        score = Number((totalScore / 100 * 25).toFixed(1));
        console.log(`Using fallback score: ${score}`);
      } else {
        console.warn("Fallback score is also 0 or failed. Score remains 0.");
        score = 0;
      }
    }

    // Ensure score is within 0-25 range and is a valid number
    score = Math.max(0, Math.min(25, isNaN(score) ? 0 : score));
    content += `\n## æ€»åˆ†\n${score}åˆ†`; // Use standard markdown for Total Score header

    console.log(`Final score calculated: ${score}`); // Log final score
    return { score, content };

  } catch (error) {
    console.error("Error generating score:", error);
    if (fullResponseText) {
      console.error("Partial AI response received before error:", fullResponseText);
    }
    enqueue({ type: 'error', message: 'ç”Ÿæˆè¯„åˆ†å¤±è´¥', error: String(error) });
    throw error;
  }
}

// --- generateTitle and generateIcon remain unchanged ---

/**
 * Generates a title for the essay continuation based on the original text.
 * @param originalText The original text prompt.
 * @param enqueue Function to send progress updates via the stream.
 * @returns The generated title string.
 */
export async function generateTitle(
  originalText: string,
  enqueue: EnqueueFunction
): Promise<string> {
  enqueue({ type: 'progress', message: 'æ­£åœ¨ç”Ÿæˆæ ‡é¢˜...' });

  try {
    // Use a faster model for title generation
    const fastModel = openai('@cf/meta/llama-3.1-8b-instruct-fast'); // Assuming this model exists and is fast

    const { text: titleResponse } = await generateText({
      model: fastModel,
      messages: [
        {
          role: 'system',
          content: `ç”¨æˆ·å°†æä¾›ç»™ä½ ä¸€é“è¯»åç»­å†™é¢˜ï¼Œè¯·ä½ åˆ†æé¢˜ç›®å†…å®¹ï¼Œå¹¶æ€»ç»“å‡ºä¸€ä¸ªæ ‡é¢˜ï¼Œè¾“å‡ºæ—¶ä»…åŒ…å«ä¸€è¡Œè¯¥æ ‡é¢˜ï¼Œä¸å…è®¸å‡ºç°å…¶ä»–å­—ç¬¦æˆ–æ³¨é‡Šã€‚`,
        },
        {
          role: 'user',
          content: `è¯»åç»­å†™é¢˜ï¼š\n${originalText}`,
        }
      ],
      temperature: 0.5 // Allow some creativity for title
    });

    return titleResponse.trim(); // Return the generated title, trimmed

  } catch (error) {
    console.error("Error generating title:", error);
    enqueue({ type: 'error', message: 'ç”Ÿæˆæ ‡é¢˜å¤±è´¥', error: String(error) });
    throw error; // Re-throw
  }
}

/**
 * Generates an emoji icon for the essay continuation based on the original text.
 * @param originalText The original text prompt.
 * @param enqueue Function to send progress updates via the stream.
 * @returns The generated emoji icon string.
 */
export async function generateIcon(
  originalText: string,
  enqueue: EnqueueFunction
): Promise<string> {
  enqueue({ type: 'progress', message: 'æ­£åœ¨ç”Ÿæˆå›¾æ ‡...' });

  try {
    // Use a faster model for icon generation
    const fastModel = openai('@cf/meta/llama-3.1-8b-instruct-fast'); // Assuming this model exists and is fast

    const { text: icon } = await generateText({
      model: fastModel,
      messages: [
        {
          role: 'system',
          content: `ç”¨æˆ·å°†æä¾›ç»™ä½ ä¸€é“è¯»åç»­å†™é¢˜ï¼Œè¯·ä½ åˆ†æé¢˜ç›®å†…å®¹ï¼Œå¹¶è¾“å‡ºä¸€ä¸ªä½ è®¤ä¸ºä¸ä¹‹ç›¸å…³çš„emojiå­—ç¬¦ï¼ˆåªèƒ½ä¸€ä¸ªå­—ç¬¦ï¼Œä¾‹å¦‚ğŸ“ï¼‰ï¼Œå¹¶ä¸”åœ¨è¾“å‡ºæ—¶ä¸å…è®¸å‡ºç°å…¶ä»–å­—ç¬¦æˆ–æ³¨é‡Šã€‚`,
        },
        {
          role: 'user',
          content: `è¯»åç»­å†™é¢˜ï¼š\n${originalText}`,
        }
      ],
      temperature: 0.3 // Keep it focused
    });

    // Basic validation for a single character emoji
    const trimmedIcon = icon.trim();
    // Improved emoji check using unicode property escape and length check
    if (trimmedIcon && (trimmedIcon.length === 1 || /\p{Emoji}/u.test(trimmedIcon))) {
      // Return the first grapheme cluster if multiple emojis are returned accidentally
      const firstEmoji = [...trimmedIcon][0];
      return firstEmoji;
    } else {
      // If AI didn't return a single emoji, provide a default or log a warning
      console.warn(`AI did not return a single valid emoji for icon (received: '${icon}'), returning default.`);
      return "ğŸ“„"; // Default icon
    }

  } catch (error) {
    console.error("Error generating icon:", error);
    enqueue({ type: 'error', message: 'ç”Ÿæˆå›¾æ ‡å¤±è´¥', error: String(error) });
    // Return a default icon on error
    return "ğŸ“„";
  }
}